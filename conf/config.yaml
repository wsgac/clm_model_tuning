output_dir: "tuned-model"
load_dir: "epoch_0_most_recent" #"model_with_best_eval"

bittensor:
  network: "nobunaga" #"nakamoto"

dataset:
  # either 'bittensor', a local path, or one from huggingface
  name: "bittensor"
  config_name: null # necessary for huggingface datasets
  num_batches: 200000
  block_size: 256 # if null, defaults to bittensor's validator sequence length.

  data_dir: "train_data"
  n_samples: 300000
  n_shards: 1
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false # only really necessary when loading a local .txt file

model:
  name: EleutherAI/pythia-12b
  config_name: null

tokenizer:
  name: EleutherAI/pythia-12b
  use_fast: true
  preprocessing_num_workers: null
  pad_token: "[PAD]"

training:
  seed: 102
  val_split_percent: 5

  # if null these both default to bittensor's validator batch size
  train_batch_size: 12
  eval_batch_size: 16

  learning_rate: 5e-5
  weight_decay: 0
  num_epochs: 50
  max_train_steps: 400000
  gradient_accumulation_steps: 64
  lr_scheduler: "cosine_with_restarts" # ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
  lr_warmup_steps: 100
  eval_every: 25

  checkpoint:
    resume_from_checkpoint: 0 # integer representing which checkpoint to load from, or <= 0 to not
    every_n_steps: 20

tracking:
  enabled: false
  report_to: "all"
